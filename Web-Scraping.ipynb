{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scarping a basic html web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\okori\\\\Documents\\\\Workspace'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing zipfile library\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with zipped file object in python\n",
    "with zipfile.ZipFile('rt-html.zip', 'r') as myzip:\n",
    "    myzip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <base href=\"/knowledgebase\"/>\n",
      "  <title>\n",
      "   Blackbaud\n",
      "  </title>\n",
      "  <link href=\"https://sky.blackbaudcdn.net/skyuxapps/host-assets/assets/apple-touch-icon-v1.4c56b890293b1d7d846253a114524ca11a44972a.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n",
      "  <link href=\"https://sky.blackbaudcdn.net/skyuxapps/host-assets/assets/favicon-32x32-v1.c216706904f5f5082332b9fe0cfb6a78b5a478c3.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n",
      "  <link href=\"https://sky.blackbaudcdn.net/skyuxapps/host-assets/assets/favicon-16x16-v1.0ffada4882ac2cd81f1473259aec3032e91a66fc.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n",
      "  <link color=\"#71bf43\" href=\"https://sky.blackbaudcdn.net/skyuxapps/host-assets/assets/safari-pinned-tab-v1.efc179d39890c5b05990bcd177971c834cb3f447.svg\" rel=\"mask-icon\"/>\n",
      "  <link href=\"/site.webmanifest\" rel=\"manifest\"/>\n",
      "  <script crossorigin=\"anonymous\" integrity=\"sha384-mZmf2gV3R8DSshmB1QcqmKq1gW8LA8y630uVOF2zecJMQb7lOBXYMtEQnTBesFu5\" nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\" src=\"https://sky.blackbaudcdn.net/static/notifications-client/1.4.1/notifications-client.global.min.js\">\n",
      "  </script>\n",
      "  <script nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\">\n",
      "   (window.BBNotificationsClient||document.write(\"\\u003Cscript src=\\u0022https://sky-pusa01.app.blackbaud.net/cdn01/static/notifications-client/1.4.1/notifications-client.global.min.js\\u0022 integrity=\\u0022sha384-mZmf2gV3R8DSshmB1QcqmKq1gW8LA8y630uVOF2zecJMQb7lOBXYMtEQnTBesFu5\\u0022 crossorigin=\\u0022anonymous\\u0022 nonce=\\u0022AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\\u0022\\u003E\\u003C/script\\u003E\"));\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" integrity=\"sha384-ULTDCz2nCZmkUsPpQ7u86DyZfdxkWHnpJH3k9/QZ+TGT/FZ1nxrxqkgMPk7qNbE1\" nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\" src=\"https://sky.blackbaudcdn.net/static/auth-client/2.60.0/auth-client.global.min.js\">\n",
      "  </script>\n",
      "  <script nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\">\n",
      "   (window.BBAuthClient||document.write(\"\\u003Cscript src=\\u0022https://sky-pusa01.app.blackbaud.net/cdn01/static/auth-client/2.60.0/auth-client.global.min.js\\u0022 integrity=\\u0022sha384-ULTDCz2nCZmkUsPpQ7u86DyZfdxkWHnpJH3k9/QZ\\u002BTGT/FZ1nxrxqkgMPk7qNbE1\\u0022 crossorigin=\\u0022anonymous\\u0022 nonce=\\u0022AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\\u0022\\u003E\\u003C/script\\u003E\"));\n",
      "  </script>\n",
      "  <script nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\">\n",
      "   window.SKYUX_HOST = {\n",
      "      acceptLanguage: 'en-US,en;q=0.9', \n",
      "      theming: { \n",
      "        serviceIdMap: {\"faith\":\"modern\",\"skyux-modern\":\"modern\",\"tcs\":\"modern\"},\n",
      "        services: {\"faith\":{\"app\":\"modern\",\"omnibar\":\"modern\"},\"renxt\":{\"app\":\"default\",\"omnibar\":\"default\"},\"skyux-modern\":{\"app\":\"modern\",\"omnibar\":\"modern\"},\"tcs\":{\"app\":\"modern\",\"omnibar\":\"modern\"}}\n",
      "      },\n",
      "      bannerHeight: 0\n",
      "    };\n",
      "  </script>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://sky.blackbaudcdn.net/skyuxapps/knowledgebase/styles.89090c27964853d1.css\" integrity=\"sha384-5yFlPzRvnRFS0R8pp2fp2thtm5W7scEqH5+uDkV/r549PkhZXR4108cde1OSjGim\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <meta class=\"sky-pages-ready-styles-89090c27964853d1-css\" content=\"\" name=\"x-stylesheet-fallback-test\"/>\n",
      "  <script nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\">\n",
      "   !function(a,b,c,d){var e,f=document,g=f.getElementsByTagName(\"SCRIPT\"),h=g[g.length-1].previousElementSibling,i=f.defaultView&&f.defaultView.getComputedStyle?f.defaultView.getComputedStyle(h):h.currentStyle;if(i&&i[a]!==b)for(e=0;e<c.length;e++)f.write('<link href=\"'+c[e]+'\" '+d+\"/>\")}(\"visibility\",\"hidden\",[\"https://sky-pusa01.app.blackbaud.net/cdn01/skyuxapps/knowledgebase/styles.89090c27964853d1.css\"], \"rel=\\u0022stylesheet\\u0022 type=\\u0022text/css\\u0022 integrity=\\u0022sha384-5yFlPzRvnRFS0R8pp2fp2thtm5W7scEqH5\\u002BuDkV/r549PkhZXR4108cde1OSjGim\\u0022 crossorigin=\\u0022anonymous\\u0022 \");\n",
      "  </script>\n",
      " </head>\n",
      " <body>\n",
      "  <app-root>\n",
      "   <style>\n",
      "    * {margin: 0;padding:0;}.mask-loading {margin: auto;position: absolute;top: 0;right: 0;bottom: 0;left: 0;}.spinner {width: 50px;height: 50px;margin-top: -25px;margin-left: -25px;position: absolute;top: 50%;left: 50%;}.double-bounce1, .double-bounce2 {width: 100%;height: 100%;border-radius: 50%;background-color: #71bf43;opacity: 0.6;position: absolute;top: 0;left: 0;-webkit-animation: sk-bounce 2.0s infinite ease-in-out;animation: sk-bounce 2.0s infinite ease-in-out;}.double-bounce2 {-webkit-animation-delay: -1.0s;animation-delay: -1.0s;}@-webkit-keyframes sk-bounce {0%, 100% { -webkit-transform: scale(0.0) }50% { -webkit-transform: scale(1.0) }}@keyframes sk-bounce {0%, 100% {transform: scale(0.0);-webkit-transform: scale(0.0);} 50% {transform: scale(1.0);-webkit-transform: scale(1.0);}}\n",
      "   </style>\n",
      "   <div class=\"mask-loading\">\n",
      "    <div class=\"spinner\">\n",
      "     <div class=\"double-bounce1\">\n",
      "     </div>\n",
      "     <div class=\"double-bounce2\">\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </app-root>\n",
      "  <script crossorigin=\"anonymous\" integrity=\"sha384-uXDIBHTHpzcvnOO8RANs/XnbMaqYFNfvMctTjF4Iz/KPtnNMVl2ATr/uDFC67x20\" nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\" src=\"https://sky.blackbaudcdn.net/skyuxapps/knowledgebase/runtime.ed94a6add0db0187.js\">\n",
      "  </script>\n",
      "  <script nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\">\n",
      "   (window.SKY_PAGES_READY_RUNTIME_ED94A6ADD0DB0187_JS||document.write(\"\\u003Cscript src=\\u0022https://sky-pusa01.app.blackbaud.net/cdn01/skyuxapps/knowledgebase/runtime.ed94a6add0db0187.js\\u0022 integrity=\\u0022sha384-uXDIBHTHpzcvnOO8RANs/XnbMaqYFNfvMctTjF4Iz/KPtnNMVl2ATr/uDFC67x20\\u0022 crossorigin=\\u0022anonymous\\u0022 nonce=\\u0022AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\\u0022\\u003E\\u003C/script\\u003E\"));\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" integrity=\"sha384-wZWgPIskunrC+SxxmuIymMgS2/17qFKibHSG2GDJrrxHtayNYpNqYMXpoN9QKN81\" nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\" src=\"https://sky.blackbaudcdn.net/skyuxapps/knowledgebase/polyfills.b86cde26e4a13d8a.js\">\n",
      "  </script>\n",
      "  <script nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\">\n",
      "   (window.SKY_PAGES_READY_RUNTIME_ED94A6ADD0DB0187_JS&&window.SKY_PAGES_READY_POLYFILLS_B86CDE26E4A13D8A_JS||document.write(\"\\u003Cscript src=\\u0022https://sky-pusa01.app.blackbaud.net/cdn01/skyuxapps/knowledgebase/polyfills.b86cde26e4a13d8a.js\\u0022 integrity=\\u0022sha384-wZWgPIskunrC\\u002BSxxmuIymMgS2/17qFKibHSG2GDJrrxHtayNYpNqYMXpoN9QKN81\\u0022 crossorigin=\\u0022anonymous\\u0022 nonce=\\u0022AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\\u0022\\u003E\\u003C/script\\u003E\"));\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" integrity=\"sha384-McLYH3F92MgzxLRe1+7J6zcHPk2/eBYh8H4nMm8R3j6EUnuJ0akcw43ObDQcaaY1\" nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\" src=\"https://sky.blackbaudcdn.net/skyuxapps/knowledgebase/main.42dba3df423d630c.js\">\n",
      "  </script>\n",
      "  <script nonce=\"AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\">\n",
      "   (window.SKY_PAGES_READY_RUNTIME_ED94A6ADD0DB0187_JS&&window.SKY_PAGES_READY_POLYFILLS_B86CDE26E4A13D8A_JS&&window.SKY_PAGES_READY_MAIN_42DBA3DF423D630C_JS||document.write(\"\\u003Cscript src=\\u0022https://sky-pusa01.app.blackbaud.net/cdn01/skyuxapps/knowledgebase/main.42dba3df423d630c.js\\u0022 integrity=\\u0022sha384-McLYH3F92MgzxLRe1\\u002B7J6zcHPk2/eBYh8H4nMm8R3j6EUnuJ0akcw43ObDQcaaY1\\u0022 crossorigin=\\u0022anonymous\\u0022 nonce=\\u0022AVWq5CEASZdpXKVVJR9NxyD8F/DJwKH9\\u0022\\u003E\\u003C/script\\u003E\"));\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# Working with file objects in python\n",
    "\n",
    "with open(\"C:/Users/okori\\Documents/Workspace/How to save a web page in HTML format.html\", 'r') as html_file:\n",
    "    content = html_file.read()\n",
    "    #print(content)\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    #text_tags = soup.find_all('script')\n",
    "    print(soup.prettify())\n",
    "\n",
    "    #for items in text_tags:\n",
    "        #print(items.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Company Name: \n",
      "SuryaInformaticsSolutionsPvt.Ltd.\n",
      "\n",
      "\n",
      "Required Skills: \n",
      "python,webtechnologies,linux,mobile,mysql,angularjs,javascript\n",
      "\n",
      "\n",
      "Published Date: Posted few days ago\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Requests library is requesting information from a website. Using .get method is to get a specific information from a website.\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=').text\n",
    "# print(html_text)\n",
    "# Creating a beautifulSoup Instance and providing the html as a text variable (information to scrap)\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "# Accessing the parent tag (element of 'li' tag) containing the header tag (h3) with the text we are looking for.\n",
    "jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "# print(jobs)\n",
    "\n",
    "# In order to work with this scraping project, it makes sense to work with just one job element.\n",
    "job = soup.find('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "# Finding the company name of the specific job post. The company name is inside the 'li' tag and is also found under the 'h3' tag with class name 'class=\"joblist-comp-name\"'\n",
    "company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '') # We are searching for the job company posting (element) only inside the job object\n",
    "# print(company_name)\n",
    "# .replace method was used to replace whitespaces with nothing.\n",
    "\n",
    "# searching for elements that includes the skills requirements\n",
    "skills = job.find('span', class_=\"srp-skills\").text.replace(' ', '')\n",
    "# print(skills)\n",
    "# Searching for the tag with the job published date element.\n",
    "published_date = job.find('span', class_=\"sim-posted\").span.text\n",
    "# print(published_date)\n",
    "\n",
    "print(f\"\"\"\n",
    "Company Name: {company_name}\n",
    "Required Skills: {skills}\n",
    "Published Date: {published_date}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for the jobs posted on the webpage with published date not showing 'few'\n",
    "# We want our functionality to stop executing if our code published date code does not contain the word 'few'\n",
    "# Lets ouput all the jobs from the first page\n",
    "jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "for job in jobs:\n",
    "    published_date = job.find('span', class_=\"sim-posted\").span.text\n",
    "    if 'few' in published_date: # using IF statement to check if the word 'few' is inside the published date text\n",
    "        company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '') # We are searching for the job company posting (element) only inside the job object\n",
    "        skills = job.find('span', class_=\"srp-skills\").text.replace(' ', '')\n",
    "\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        Company Name: {company_name}\n",
    "        Required Skills: {skills}\n",
    "        \"\"\")\n",
    "        print(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put some skill that you are not familiar with\n",
      "Filtering out django\n",
      "File saved: 0\n",
      "File saved: 5\n",
      "File saved: 7\n",
      "File saved: 9\n",
      "File saved: 11\n",
      "File saved: 13\n",
      "File saved: 18\n",
      "File saved: 21\n",
      "Waiting 10 minutes...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    # Using input function to catch skills the use does not own\n",
    "print(\"Put some skill that you are not familiar with\")\n",
    "unfamiliar_skill = input('>')\n",
    "print(f\"Filtering out {unfamiliar_skill}\")\n",
    "\n",
    "def find_jobs():\n",
    "    html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=').text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    # Using the beautiful soup library methods to find all tags with job elements in them with a for loop\n",
    "    jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "\n",
    "    for index, job in enumerate(jobs):\n",
    "        published_date = job.find('span', class_=\"sim-posted\").span.text\n",
    "        if 'few' in published_date: # using IF statement to check if the word 'few' is inside the published date text\n",
    "            company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '') # We are searching for the job company posting (element) only inside the job object\n",
    "            skills = job.find('span', class_=\"srp-skills\").text.replace(' ', '')\n",
    "            more_info = job.header.h2.a['href']\n",
    "            # using an IF statement to filter out skills inputed by the user as unfamiliar skills\n",
    "            if unfamiliar_skill not in skills:\n",
    "                with open(f'posts/{index}.txt', 'w') as f:\n",
    "                    f.write(f\"Company Name: {company_name.strip()} \\n\")\n",
    "                    f.write(f\"Required Skills: {skills.strip()} \\n\")\n",
    "                    f.write(f\"More info: {more_info}\")\n",
    "                print(f'File saved: {index}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        find_jobs()\n",
    "        time_wait = 10\n",
    "        print(f'Waiting {time_wait} minutes...')\n",
    "        time.sleep(time_wait * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\okori\\\\Documents\\\\Workspace'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the dictionary to a text file using the json library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuryaInformaticsSolutionsPvtLtd\n",
      "['python', 'webtechnologies', 'linux', 'mobile', 'mysql', 'angularjs', 'javascript'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "MACKENZIEMODERNITSOLUTIONSPVTLTD\n",
      "['DataScience', 'Matplotlib', 'Git', 'Panda', 'Aws', 'Numpy', 'PythonScripting', 'BusinessProcess', 'Testing'], ['Posted3daysago']\n",
      "\n",
      "\n",
      "PerfiosSoftware\n",
      "['python', 'java', 'scala'], ['Posted2daysago']\n",
      "\n",
      "\n",
      "MaxgenTechnologies\n",
      "['gujarat', 'python', 'winterinternship'], ['Posted6daysago']\n",
      "\n",
      "\n",
      "TriadssTechSolutions\n",
      "['python', 'django', 'html5', 'javascript'], ['Posted1dayago']\n",
      "\n",
      "\n",
      "IvanInfotechPvtLtd\n",
      "['rest', 'python', 'security', 'debugging'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "qualitythought\n",
      "['rest', 'python', 'django', 'informationtechnology'], ['Posted2daysago']\n",
      "\n",
      "\n",
      "eastindiasecuritiesltd\n",
      "['python', 'hadoop', 'machinelearning'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "TandAHRSolutions\n",
      "['Python', 'Django', 'Flask'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "JobsLoConsultants\n",
      "['python', 'linux', 'windows', 'sql'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "appypie\n",
      "['python', 'django', 'html5', 'javascript'], ['Posted6daysago']\n",
      "\n",
      "\n",
      "arttechnologyandsoftwareindiapvtltd\n",
      "['rest', 'python', 'database', 'django', 'api'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "sjainventures\n",
      "['python', 'webdeveloper', 'webservices'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "systango\n",
      "['python', 'django', 'javascript', 'webprogramming'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "RouteMobileLimited\n",
      "['python', 'django', 'postgresql', 'mysql', 'docker'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "youngmindstechnologysolutionspvtltd\n",
      "['python', 'html5', 'storage', 'javascript', 'security', 'django'], ['Posted2daysago']\n",
      "\n",
      "\n",
      "PolarisConsulting&ServicesLtd\n",
      "['python', 'django', 'apache', 'devops'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "APPLYCUPHIRINGSOLUTIONSLLP\n",
      "['python', 'django', 'html5', 'javascript'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "onsinteractivesolutionspvtltd\n",
      "['python', 'django', 'html5', 'javascript'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "AnanthaCyberTechPvtLimited\n",
      "['python', 'c', 'c', 'informationtechnology'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "DREAMAJAXTECHNOLOGIES\n",
      "['python', 'django', 'api', 'sql', 'nosql'], ['Postedfewdaysago']\n",
      "\n",
      "\n",
      "day1technologies\n",
      "['rest', 'python', 'django', 'git', 'postgresql', 'sql', 'docker'], ['Posted6daysago']\n",
      "\n",
      "\n",
      "pofitechnologiespvtltd\n",
      "['python', 'django', 'html5', 'oops', 'javascript'], ['Posted6daysago']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using dictionary as a data container to store the scraped job details from the webpage\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=').text\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "job_posting_dict = {}\n",
    "\n",
    "jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "company_names = []\n",
    "for job in jobs:\n",
    "    company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '').strip().replace('.', '').replace('\\r\\n', '')    # We are searching for the job company posting (element) only inside the job object\n",
    "    skills = job.find('span', class_=\"srp-skills\").text.replace(' ', '').strip().split(',')\n",
    "    published_date = job.find('span', class_=\"sim-posted\").span.text.replace(' ', '').strip()\n",
    "    \n",
    "    if '(MoreJobs)' in company_name:\n",
    "        company_name = company_name.replace('(MoreJobs)', '')\n",
    "\n",
    "\n",
    "\n",
    "    #job_posting_dict[f\"{company_name}\"] = f\"[{skills}],{published_date}\"\n",
    "    \n",
    "    # Alternatively\n",
    "    job_posting_dict.update({f\"{company_name}\": f\"{skills}, {[published_date]}\"})\n",
    "\n",
    "#print(job_posting_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#job_posting_dict1 = job_posting_dict.copy()\n",
    "\n",
    "# Writing the generated dictionary to a txt file\n",
    "import json\n",
    "\n",
    "with open('job_archives.txt', 'w') as job_file:\n",
    "    job_file.write(json.dumps(job_posting_dict))\n",
    "\n",
    "\n",
    "# Obtaining the details about the job postings\n",
    "for key, value in job_posting_dict.items():\n",
    "    print(key), print(value), print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in your desired job level and job category\n",
      "Filtering out job adverts with Senior Level and Full Time specifications:\n",
      "Filed saved: 0\n",
      "Filed saved: 18\n",
      "Filed saved: 26\n",
      "Filed saved: 28\n",
      "Filed saved: 29\n",
      "Waiting 10 minutes...\n"
     ]
    }
   ],
   "source": [
    "# Web-scraping job adverts from Zippia Career Expert website\n",
    "\n",
    "\n",
    "#importing the open source libraries and packages to be used\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Defining a function for this job web scraping script\n",
    "url = \"https://www.zippia.com/web-developer-tallahassee-fl-jobs/\"\n",
    "\n",
    "print(\"Type in your desired job level and job category\")\n",
    "target_job_level = input('>')\n",
    "target_job_category = input('>')\n",
    "\n",
    "print(f\"Filtering out job adverts with {target_job_level} and {target_job_category} specifications:\")\n",
    "\n",
    "def find_jobADS():\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    jobs = soup.find_all('div', class_=\"d-flex col-10 col-sm-11 flex-column newJobSearchUI_card-content__nDbJ_ pl-2 pl-sm-4\")\n",
    "\n",
    "\n",
    "    for index, job in enumerate(jobs):\n",
    "        span_tag = job.find(\"div\", class_=\"d-flex flex-wrap newJobSearchUI_job-tags__1WF6Y\").find_all('span')\n",
    "\n",
    "        for spantag in span_tag:\n",
    "            for tag in spantag:\n",
    "                #print(tag)\n",
    "                if tag in ('Senior Level', 'Junior Level', 'Mid Level', 'Entry Level'):\n",
    "                    job_level = tag.text.strip()\n",
    "                #print(job_level)\n",
    "\n",
    "                elif tag in ('Full Time', 'Part Time', 'Internship'):\n",
    "                    job_category = tag.text.strip()\n",
    "\n",
    "                elif tag in (\"Bachelors Preferred\", \"Bachelors Required\", \"High School Diploma Preferred\", \"Masters Preferred\", \"Masters Required\", \"Doctorate Preferred\", \"Associate Required\"):\n",
    "                    qualification = tag.text.strip()\n",
    "\n",
    "                else:\n",
    "                    job_level = \"job level not specified\"\n",
    "                    job_category = \"job category not specified\"\n",
    "                    qualification = \"qualification is not specified\"\n",
    "\n",
    "                job_title = job.find('h3', class_=\"newJobSearchUI_job-card--job-title__E68J0 mb-0 line-clamp-2\").text.replace(' (DAY 1 ON-SITE WORK)', '')\n",
    "                company_name = job.find('div', class_=\"newJobSearchUI_job-card--company-name__aoOaL\").text\n",
    "                job_specification = job.find('div', class_=\"newJobSearchUI_job-card--company-name__aoOaL newJobSearchUI_job-card--location__UGwfb\").text\n",
    "                brief_information = job.find('div', class_=\"lightTextStyle2 newJobSearchUI_job-description__pOQor\").text.replace('\\n', '')\n",
    "                payment_amount = job.find('div', class_=\"d-flex justify-content-between align-items-center newJobSearchUI_salary-postdate__mbHB9\").find('div', class_=\"newJobSearchUI_job-card--company-name__aoOaL newJobSearchUI_job-card--location__UGwfb\").text\n",
    "                date_of_job_post = job.find('div', class_=\"d-flex justify-content-between align-items-center newJobSearchUI_salary-postdate__mbHB9\").find('div', class_=\"d-block d-lg-none newJobSearchUI_job-card--company-name__aoOaL newJobSearchUI_job-card--location__UGwfb\").text\n",
    "        \n",
    "        if target_job_level == job_level and target_job_category == job_category:\n",
    "            with open(f\"jobadverts/{index}.txt\", 'w') as f:\n",
    "                f.write(f\"Job title: {job_title.strip()} \\n\")\n",
    "                f.write(f\"Company name: {company_name.strip()} \\n\")\n",
    "                f.write(f\"Job specification: {job_specification.strip()} \\n\")\n",
    "                f.write(f\"Brief information: {brief_information.strip()} \\n\")\n",
    "                f.write(f\"Payment amount: {payment_amount.strip()} \\n\")\n",
    "                f.write(f\"job level: {job_level} \\n\")\n",
    "                f.write(f\"Job category: {job_category} \\n\")\n",
    "                f.write(f\"qualification: {qualification} \\n\")\n",
    "                f.write(f\"Date of job posting: {date_of_job_post.strip()}\")\n",
    "\n",
    "            print(f\"Filed saved: {index}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        find_jobADS()\n",
    "        time_wait = 10\n",
    "        print(f'Waiting {time_wait} minutes...')\n",
    "        time.sleep(time_wait * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filed saved: 5\n",
      "Filed saved: 7\n",
      "Filed saved: 8\n",
      "Filed saved: 11\n",
      "Filed saved: 14\n",
      "Filed saved: 15\n",
      "Filed saved: 17\n",
      "Filed saved: 18\n",
      "Filed saved: 19\n",
      "Filed saved: 27\n",
      "Filed saved: 28\n",
      "Filed saved: 29\n",
      "Filed saved: 31\n",
      "Filed saved: 32\n"
     ]
    }
   ],
   "source": [
    "find_jobADS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Web APIs using Python Requests and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mentalist premiered on 2008-09-23\n",
      "Patrick Jane, an independent consultant with the California Bureau of Investigation (CBI), has a remarkable track record for solving serious crimes by using his razor sharp skills of observation. Within the Bureau, Jane is notorious for his blatant lack of protocol and his semi-celebrity past as a psychic medium, whose paranormal abilities he now admits he feigned. No-nonsense Senior Agent Teresa Lisbon openly resists having Jane in her unit and alternates between reluctantly acknowledging Jane's usefulness and blasting him for his theatrics, narcissism, and dangerous lack of boundaries. Lisbon's team includes agents Kimball Cho, Wayne Rigsby and rookie member Grace Van Pelt, who all think Jane's a loose cannon but admire his charm and knack for clearing cases.\n"
     ]
    }
   ],
   "source": [
    "# Pulling data from a TVMaze web API using requests and json module\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "url = \"https://api.tvmaze.com/singlesearch/shows\"\n",
    "show = input(f\"Please input a show name. >\")\n",
    "parameters = {\"q\" : \"The Mentalist\"}\n",
    "\n",
    "response = requests.get(url, parameters)\n",
    "\n",
    "if response.status_code  == 200:\n",
    "    data = json.loads(response.text)\n",
    "    #pprint.pprint(data)\n",
    "\n",
    "    name = data['name']\n",
    "    premiered = data['premiered']\n",
    "    summary = data['summary']\n",
    "\n",
    "    print(f\"{name} premiered on {premiered}\")\n",
    "    print(f\"{summary.lstrip('<p>').rstrip('</p>')}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error : {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
